{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WEB SCARPING AND WEB CRAWLING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "resposta = get(\"https://www.python.org/jobs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\gilma\\.jupyter\\jupyter\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gilma\\.jupyter\\jupyter\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\gilma\\.jupyter\\jupyter\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in c:\\users\\gilma\\.jupyter\\jupyter\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\gilma\\.jupyter\\jupyter\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\gilma\\.jupyter\\jupyter\\lib\\site-packages (from html5lib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = BeautifulSoup(resposta.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python Job Board | Python.org'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OBTENDO DADOS DESORDENADOS\n",
    "title = tags.find(\"title\")\n",
    "title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Get exploring!',\n",
       " '\\n                \\n                    New\\n                    \\n                    Infrastructure Engineer\\n\\t\\t    Python Software Foundation\\n                \\n                Remote, Worldwide\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Python Data Engineer\\n\\t\\t    Tech City Labs\\n                \\n                Reading, UK\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Senior Backend Engineer (Python/Django)\\n\\t\\t    Multi Media LLC\\n                \\n                Los Angeles, San Francisco, Irvine, CA or Remote, United States\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Full Stack Python Engineer\\n\\t\\t    Multi Media LLC\\n                \\n                Los Angeles, San Francisco, Irvine, CA or Remote, United States\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Senior Software Engineer, Platform - Type System\\n\\t\\t    C3 AI\\n                \\n                Redwood City, California, United States\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Computational Seismology Postdoc\\n\\t\\t    Los Alamos National Lab\\n                \\n                Los Alamos, NM, United States\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    PFS INSTRUMENT SOFTWARE ENGINEER – ID# 222151\\n\\t\\t    National Astronomical Observatory of Japan\\n                \\n                Hilo, HI, USA\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Full Stack Developer\\n\\t\\t    In2Books, Inc., a division of Cricket Media®\\n                \\n                McLean, VA, U.S.\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    [Remote] Backend - Django, Celery, DRF, RabbitMQ, Redis, Postgres 13\\n\\t\\t    Friend Finder Networks\\n                \\n                Miami, Florida, United States\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Software Engineer –\\xa0Data Engineering, Plates, Charts & Docs\\n\\t\\t    ForeFlight, LLC\\n                \\n                Austin / Houston / Portland, ME / Remote, US\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Django Engineer\\n\\t\\t    Potato London Ltd.\\n                \\n                London, United Kingdom\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Senior Back-end Developer (Python, SQL, DevOps, and Cloud Architect)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Back-end Systems Engineer (Python, Docker, and Kubernetes)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Senior Back-end Software Engineer (Cloud Platforms, Node.js, Python, and Java)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Back-end Staff Software Engineer (Node.js, Python, Java, and Cloud Platforms)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Senior Back-end Engineer (Python, SQL, and Terraform)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Software Developer 2\\n\\t\\t    University of Connecticut Student Affairs IT\\n                \\n                Storrs, CT, US\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Software Developer 1 (two positions)\\n\\t\\t    University of Connecticut Student Affairs IT\\n                \\n                Storrs, CT, US\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Web Application Developer for a Video Archiving System\\n\\t\\t    The Interaction Consortium\\n                \\n                Sydney, New South Wales, Australia\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Advanced Python Engineer (Remote / Hybrid US Pacific)\\n\\t\\t    Research Affiliates\\n                \\n                Newport Beach, CA, USA\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Senior Python Developers\\n\\t\\t    Tuna Technology\\n                \\n                Butwal, Rupandehi, Nepal, Lumbini (State 5), Nepal\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Full Stack Developer (Python, APIs, and Cloud)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote QA Developer (Java, Python, and JavaScript)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Remote Python Developer (No Gender Bias)\\n\\t\\t    Turing.com\\n                \\n                Palo Alto, California, United States of America\\n            ',\n",
       " '\\n                \\n                    New\\n                    \\n                    Python software engineer (Remote)\\n\\t\\t    Photon Commerce\\n                \\n                San Francisco, Remote, USA\\n            ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitles = tags.find_all(\"h2\")\n",
    "[h2.text for h2 in subtitles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Infrastructure Engineer',\n",
       " 'Python Data Engineer',\n",
       " 'Remote Senior Backend Engineer (Python/Django)',\n",
       " 'Remote Full Stack Python Engineer',\n",
       " 'Senior Software Engineer, Platform - Type System',\n",
       " 'Computational Seismology Postdoc',\n",
       " 'PFS INSTRUMENT SOFTWARE ENGINEER – ID# 222151',\n",
       " 'Full Stack Developer',\n",
       " '[Remote] Backend - Django, Celery, DRF, RabbitMQ, Redis, Postgres 13',\n",
       " 'Software Engineer –\\xa0Data Engineering, Plates, Charts & Docs',\n",
       " 'Django Engineer',\n",
       " 'Remote Senior Back-end Developer (Python, SQL, DevOps, and Cloud Architect)',\n",
       " 'Remote Back-end Systems Engineer (Python, Docker, and Kubernetes)',\n",
       " 'Remote Senior Back-end Software Engineer (Cloud Platforms, Node.js, Python, and Java)',\n",
       " 'Remote Back-end Staff Software Engineer (Node.js, Python, Java, and Cloud Platforms)',\n",
       " 'Remote Senior Back-end Engineer (Python, SQL, and Terraform)',\n",
       " 'Software Developer 2',\n",
       " 'Software Developer 1 (two positions)',\n",
       " 'Web Application Developer for a Video Archiving System',\n",
       " 'Advanced Python Engineer (Remote / Hybrid US Pacific)',\n",
       " 'Senior Python Developers',\n",
       " 'Remote Full Stack Developer (Python, APIs, and Cloud)',\n",
       " 'Remote QA Developer (Java, Python, and JavaScript)',\n",
       " 'Remote Python Developer (No Gender Bias)',\n",
       " 'Python software engineer (Remote)']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OBTENDO O TEXTO DE MANEIRA MAIS CLARA\n",
    "subtitles = tags.find_all(\"h2\", attrs = {\"class\": \"listing-company\"})\n",
    "[h2.a.text for h2 in subtitles] #Utilização da tag A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos construir um wewb crawler extraindo os links de cada vaga\n",
    "para_visitar = [\"https://www.python.org\" + h2.a[\"href\"] for h2 in subtitles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.python.org/jobs/6261/',\n",
       " 'https://www.python.org/jobs/6260/',\n",
       " 'https://www.python.org/jobs/6259/',\n",
       " 'https://www.python.org/jobs/6258/',\n",
       " 'https://www.python.org/jobs/6257/',\n",
       " 'https://www.python.org/jobs/6256/',\n",
       " 'https://www.python.org/jobs/6255/',\n",
       " 'https://www.python.org/jobs/6254/',\n",
       " 'https://www.python.org/jobs/6253/',\n",
       " 'https://www.python.org/jobs/6252/',\n",
       " 'https://www.python.org/jobs/6251/',\n",
       " 'https://www.python.org/jobs/6249/',\n",
       " 'https://www.python.org/jobs/6248/',\n",
       " 'https://www.python.org/jobs/6247/',\n",
       " 'https://www.python.org/jobs/6246/',\n",
       " 'https://www.python.org/jobs/6245/',\n",
       " 'https://www.python.org/jobs/6244/',\n",
       " 'https://www.python.org/jobs/6243/',\n",
       " 'https://www.python.org/jobs/6242/',\n",
       " 'https://www.python.org/jobs/6241/',\n",
       " 'https://www.python.org/jobs/6240/',\n",
       " 'https://www.python.org/jobs/6239/',\n",
       " 'https://www.python.org/jobs/6238/',\n",
       " 'https://www.python.org/jobs/6237/',\n",
       " 'https://www.python.org/jobs/6236/']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_visitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = []                 #depois estudar em novas paginas web\n",
    "for pv in para_visitar:\n",
    "    acesso = get(pv)\n",
    "    sites.append(acesso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]\n"
     ]
    }
   ],
   "source": [
    "print(sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached Scrapy-2.6.1-py2.py3-none-any.whl (264 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Using cached service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Using cached cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tldextract\n",
      "  Using cached tldextract-3.2.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from scrapy) (4.6.1)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Using cached w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Using cached Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from scrapy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from scrapy) (3.1.1)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Using cached itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Processing c:\\users\\gilma\\appdata\\local\\pip\\cache\\wheels\\d1\\d7\\61\\11b5b370ee487d38b5408ecb7e0257db9107fa622412cbe2ff\\pydispatcher-2.0.5-py3-none-any.whl\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from scrapy) (5.1.2)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from scrapy) (19.1.0)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Using cached queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Using cached parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Using cached Twisted-22.2.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: six in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (20.3.0)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.0.12)\n",
      "Collecting requests-file>=1.4\n",
      "  Using cached requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.24.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.3)\n",
      "Collecting incremental>=21.3.0\n",
      "  Using cached incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Using cached Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2; platform_system == \"Windows\"\n",
      "  Using cached twisted_iocpsupport-1.0.2-cp38-cp38-win_amd64.whl (45 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (3.7.4.3)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2020.6.20)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gilma\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "Installing collected packages: pyasn1, pyasn1-modules, service-identity, jmespath, itemadapter, w3lib, cssselect, parsel, itemloaders, requests-file, tldextract, protego, PyDispatcher, queuelib, incremental, Automat, twisted-iocpsupport, constantly, hyperlink, Twisted, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-22.2.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 protego-0.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.6.1 service-identity-21.1.0 tldextract-3.2.0 twisted-iocpsupport-1.0.2 w3lib-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Spider\n",
    "\n",
    "class ex_scrapy(Spider):\n",
    "    name = \"curso python pro\"\n",
    "    start_urls = [\"http://quotes.toscrape.com/page/1\",\n",
    "                  \"http://quotes.toscrape.com/page/2\"]\n",
    "\n",
    "def parse(self, response):\n",
    "    for quote in response.css(\"div.quote\"):\n",
    "        yield{\n",
    "            \"text\": quote.css(\"span.text::text\").extract(),\n",
    "            \"author\":quote.css(\"small.author::text\").extract(),\n",
    "            \"tags\":quote.css(\"div.tags a.tag::text\").extract(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
